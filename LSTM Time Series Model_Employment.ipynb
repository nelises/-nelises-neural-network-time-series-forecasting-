{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment LSTM Model - Training (1997-2020), Training (2021-2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "import gdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://drive.google.com/uc?id=1iJ-fXzt1maahR-_YH36yFBrHQ7lYQWf9'\n",
    "\n",
    "# Download the file\n",
    "output = 'data_unemployment.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# Check the file content\n",
    "with open(output, 'r') as file:\n",
    "    content = file.read()\n",
    "    print(\"File content preview:\")\n",
    "    print(content[:500]) \n",
    "\n",
    "# Load the CSV file \n",
    "try:\n",
    "    data_unemployment = pd.read_csv(output, delimiter=',')  \n",
    "    print(data_unemployment.head())\n",
    "except pd.errors.ParserError as e:\n",
    "    print(\"Error parsing CSV file:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'ref_date' to datetime\n",
    "data_unemployment['ref_date'] = pd.to_datetime(data_unemployment['ref_date'], format='%Y')\n",
    "\n",
    "# Label encode the 'occupation_classification' column\n",
    "data_unemployment['industry_classification'] = data_unemployment['industry_classification'].astype(str)\n",
    "label_encoder = LabelEncoder()\n",
    "data_unemployment['industry_code'] = label_encoder.fit_transform(data_unemployment['industry_classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "features = ['value', 'date_ordinal', 'sex_binary', 'age_group_numeric', 'geo_code', 'industry_code']\n",
    "scaled_data = scaler.fit_transform(data_unemployment[features])\n",
    "\n",
    "# Function to create sequences for forecasting\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length, 0]  \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Define sequence length\n",
    "SEQ_LENGTH = 3 \n",
    "\n",
    "# Split the data into training and testing sets based on the date\n",
    "train_data = data_unemployment[data_unemployment['ref_date'] < '2021']\n",
    "test_data = data_unemployment[data_unemployment['ref_date'] >= '2021']\n",
    "\n",
    "# Normalize training and testing data separately\n",
    "scaled_train_data = scaler.fit_transform(train_data[features])\n",
    "scaled_test_data = scaler.transform(test_data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for training and testing\n",
    "X_train, y_train = create_sequences(scaled_train_data, SEQ_LENGTH)\n",
    "X_test, y_test = create_sequences(scaled_test_data, SEQ_LENGTH)\n",
    "\n",
    "# Define the LSTM model for forecasting\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LENGTH, X_train.shape[2])))  \n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))  \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "end = time.time()\n",
    "\n",
    "# Convert elapsed time to minutes and seconds\n",
    "elapsed_time = end - start\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = int(elapsed_time % 60)\n",
    "\n",
    "print(f\"\\nTraining time: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the smoothing function\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "# Retrieve loss and validation loss from history\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Smooth the loss curves\n",
    "smoothed_loss = smooth_curve(loss)\n",
    "smoothed_val_loss = smooth_curve(val_loss)\n",
    "\n",
    "# Plot smoothed training and validation loss\n",
    "plt.plot(range(1, len(smoothed_loss) + 1), smoothed_loss, label='Smoothed Training Loss')\n",
    "plt.plot(range(1, len(smoothed_val_loss) + 1), smoothed_val_loss, label='Smoothed Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Identify the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(smoothed_val_loss) + 1\n",
    "print(f\"Best epoch based on validation loss: {best_epoch}\")\n",
    "\n",
    "# Refine the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(SEQ_LENGTH, X_train.shape[2]))) \n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))  \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=best_epoch, batch_size=32, validation_split=0.2)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"\\nTraining time: {minutes} minutes and {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the actual values\n",
    "predictions_inv = scaler.inverse_transform(np.concatenate((predictions, np.zeros((predictions.shape[0], scaled_train_data.shape[1] - 1))), axis=1))[:, 0]\n",
    "y_test_inv = scaler.inverse_transform(np.concatenate((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], scaled_train_data.shape[1] - 1))), axis=1))[:, 0]\n",
    "\n",
    "# Compare the first few predictions with the actual values\n",
    "comparison = pd.DataFrame({'Actual': y_test_inv, 'Predicted': predictions_inv})\n",
    "print(comparison.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
